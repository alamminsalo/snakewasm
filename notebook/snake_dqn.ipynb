{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855513d-a85a-49a4-a052-e5a592cf5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import copy\n",
    "from io import BytesIO\n",
    "from IPython.display import Image, display_png, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterplot import ProgressPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17396e56-61d1-4b16-9e63-e9255c87499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snake\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, width, height):\n",
    "        self.game = snake.PyGame(width, height)\n",
    "        self.game.tick()\n",
    "        \n",
    "    def action_size(self):\n",
    "        return 3\n",
    "    \n",
    "    def state_shape(self):\n",
    "        w, h = self.game.size()\n",
    "        return (-1, w * h)\n",
    "        \n",
    "    def state(self):\n",
    "        state = np.array(self.game.state_model())\n",
    "        return state.reshape(self.state_shape()).astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.game.input_turn(action)\n",
    "        \n",
    "        score_t0 = self.game.score()\n",
    "        self.game.tick()\n",
    "\n",
    "        reward = (self.game.score() - score_t0)# * 1.01 - 0.01\n",
    "        done = self.game.done()\n",
    "\n",
    "        if done:\n",
    "            reward = -1\n",
    "\n",
    "        return self.state(), reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.reset()\n",
    "        self.game.tick()\n",
    "        return self.state()\n",
    "    \n",
    "    def score(self):\n",
    "        return self.game.score()\n",
    "    \n",
    "    def draw(self, clear=True):\n",
    "        colors = {0: 'grey', 1: 'blue', 2: 'white', 3: 'red'}\n",
    "        if clear:\n",
    "            clear_output(wait=True)\n",
    "        state = np.array(self.game.state()).reshape(self.game.size()).astype(int)\n",
    "        state = pd.DataFrame(state)\n",
    "        display(state.style.applymap(lambda v: 'background-color: %s' % colors[v]))\n",
    "\n",
    "game = Game(9, 9)\n",
    "game.state_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172d905-c2ce-406e-ac52-d09148d66388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_size, hidden_layer_size = 256, hidden_layers = 3):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.input_size = input_shape[1]\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        n = hidden_layer_size\n",
    "        \n",
    "        od = OrderedDict()\n",
    "        for i in range(hidden_layers):\n",
    "            od[f'linear_{i}'] = nn.Linear(self.input_size if i == 0 else n, n)\n",
    "            od[f'relu_{i}'] = nn.ReLU()\n",
    "        \n",
    "        self.relu_stack = nn.Sequential(od)\n",
    "        self.output = nn.Linear(n, self.output_size)\n",
    "\n",
    "        # loss function, optimizer\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.parameters(), \n",
    "            lr=0.00001,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu_stack(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.from_numpy(x)\n",
    "            pred = self(x)\n",
    "            return pred\n",
    "        \n",
    "    def batch_train(self, x, y, batch_size=64, epochs=1):\n",
    "        # Create data loaders.\n",
    "        x = torch.Tensor(x)\n",
    "        y = torch.Tensor(y)\n",
    "        ds = TensorDataset(x,y)\n",
    "        dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        size = len(dataloader.dataset)\n",
    "        self.train()\n",
    "        for _ in range(epochs):\n",
    "            for batch, (X, y) in enumerate(dataloader):\n",
    "                # Compute prediction error\n",
    "                pred = self(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # clip gradients\n",
    "                for param in self.parameters():\n",
    "                    param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "                self.optimizer.step()\n",
    "                \n",
    "NeuralNetwork((1,2,9,9), 3, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ca518-c3ae-42b1-8284-9afd57f3baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, model: NeuralNetwork, epsilon=0.99):\n",
    "        self.state_size = model.input_size\n",
    "        self.action_size = model.output_size\n",
    "        self.state_shape = model.input_shape\n",
    "        \n",
    "        self.memory = pd.DataFrame()\n",
    "        self.gamma = 0.99   # discount rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.997\n",
    "        \n",
    "        # create prediction and target networks\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "    \n",
    "    def memory_stats(self):\n",
    "        return {\n",
    "            'mean_difficulty': self.memory['difficulty'].mean(),\n",
    "            'median_difficulty': self.memory['difficulty'].median(),\n",
    "            'mean_ticks': self.memory['ticks'].mean(),\n",
    "        }\n",
    "\n",
    "    def remember(self, arr):\n",
    "        newmem = pd.DataFrame(arr, columns=['state', 'next_state', 'action','reward','done','difficulty','ticks'])\n",
    "        self.memory = pd.concat([self.memory, newmem], ignore_index=True)\n",
    "        self.memory = self.memory.tail(20_000)\n",
    "\n",
    "    def act(self, state, explore=True):\n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)#.cpu().numpy()\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch):\n",
    "        if len(self.memory) >= batch:\n",
    "            mb = self.memory.sample(batch).copy().reset_index(drop=True)\n",
    "\n",
    "            # get state, next_state as numpy arr\n",
    "            state = np.vstack(mb['state']).reshape(self.state_shape)\n",
    "            next_state = np.vstack(mb['next_state']).reshape(self.state_shape)\n",
    "\n",
    "            # expected reward\n",
    "            with torch.no_grad():\n",
    "                pred = self.target_model.predict(next_state).numpy()\n",
    "            mb['target'] = mb['reward'] + self.gamma * np.amax(pred, axis=1)\n",
    "\n",
    "            # if done, target is direct reward\n",
    "            is_done = (mb.done == True)\n",
    "            mb.loc[is_done, 'target'] = mb[is_done].reward\n",
    "\n",
    "            q = pd.DataFrame(self.model.predict(state))\n",
    "\n",
    "            # replace with mb.target, to corresponding action col\n",
    "            for action in q.columns:\n",
    "                idx = (mb.action == action)\n",
    "                q.loc[idx, action] = mb[idx]['target']\n",
    "\n",
    "            # train x,y datasets\n",
    "            X = state\n",
    "            y = q.to_numpy()\n",
    "\n",
    "            self.model.batch_train(X, y, epochs=1, batch_size=256)\n",
    "\n",
    "            if (self.epsilon > self.epsilon_min):\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    # updates target network weights\n",
    "    def update_weights(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def load(self, path):\n",
    "        store = torch.load(path)\n",
    "        self.model.load_state_dict(store['model_state'])\n",
    "        self.model.eval()\n",
    "        self.update_weights()\n",
    "        return store['score']\n",
    "\n",
    "    def save(self, path, score):\n",
    "        torch.save({\n",
    "            'model_state': self.model.state_dict(), \n",
    "            'score': score,\n",
    "        }, path)\n",
    "\n",
    "agent = DQNAgent(\n",
    "    NeuralNetwork(game.state_shape(), game.action_size(), 64),\n",
    "    epsilon=0.99\n",
    ")\n",
    "print(\"created agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54d014-0214-4e6c-b8e3-ae5d9822d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, env, \n",
    "                 mem_batch=1000,\n",
    "                 moving_scores_n = 10, \n",
    "                 episode_max_len = 500,\n",
    "                 step_episode_count = 10, \n",
    "                 epsilon=0.99,\n",
    "                 hidden_layers=3,\n",
    "                 hidden_layer_size=64,\n",
    "                 load=False,\n",
    "                ):\n",
    "        self.name = f\"nn_{hidden_layers}x{hidden_layer_size}_m{mem_batch}\"\n",
    "        \n",
    "        self.env = env\n",
    "        self.mem_batch = mem_batch\n",
    "        self.moving_scores = deque(maxlen=moving_scores_n)\n",
    "        self.episode_max_len = episode_max_len\n",
    "        self.step_episode_count = step_episode_count\n",
    "        self.best_score = 0\n",
    "        \n",
    "        # create our agent\n",
    "        self.agent = DQNAgent(\n",
    "            NeuralNetwork(env.state_shape(), env.action_size(), hidden_layers=hidden_layers, hidden_layer_size=hidden_layer_size),\n",
    "            epsilon=epsilon\n",
    "        )\n",
    "        \n",
    "        if load:\n",
    "            self.best_score = self.agent.load(f'{self.name}.pk')\n",
    "    \n",
    "    def epsilon(self):\n",
    "        return self.agent.epsilon\n",
    "    \n",
    "    def step(self, visual=False):\n",
    "        emem = deque(maxlen=self.episode_max_len)\n",
    "        scores = []\n",
    "\n",
    "        for i in range(self.step_episode_count):\n",
    "            state = self.env.reset()\n",
    "            score = 0\n",
    "            for _ in range(self.episode_max_len):\n",
    "                action = self.agent.act(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                \n",
    "                # introduce difficulty metric to keep track on what is the difficulty in the memory\n",
    "                difficulty = self.env.score()\n",
    "                ticks = self.env.game.tick_count()\n",
    "\n",
    "                # create flat array of variables from this step and add them to episode memory\n",
    "                emem.append([[state.reshape(-1)], [next_state.reshape(-1)], action, reward,  done, difficulty, ticks])\n",
    "                score += reward\n",
    "                state = next_state\n",
    "\n",
    "                if visual:\n",
    "                    self.env.draw()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        # give episode memory list to agent\n",
    "        self.agent.remember(emem)\n",
    "\n",
    "        score_median = np.median(scores)\n",
    "        self.moving_scores.append(score_median)\n",
    "        moving_score = np.mean(self.moving_scores)\n",
    "\n",
    "        if score_median > self.best_score:\n",
    "            self.best_score = score_median\n",
    "            self.agent.save(f'{self.name}.pk', self.best_score)\n",
    "\n",
    "        if e % 20 == 0:\n",
    "            self.agent.update_weights()\n",
    "        \n",
    "        # train the agent\n",
    "        self.agent.replay(self.mem_batch)\n",
    "            \n",
    "        return score_median, moving_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbee84-5bf3-42b6-b4bc-0e3c7a4e249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bunch of different agents and train them\n",
    "EPISODES = 3000\n",
    "\n",
    "trainers = [\n",
    "    Trainer(\n",
    "        Game(9, 9),\n",
    "        hidden_layer_size=512,\n",
    "        hidden_layers=3,\n",
    "        mem_batch=4096,\n",
    "        load=True,\n",
    "        epsilon=0.1\n",
    "    ),\n",
    "]\n",
    "\n",
    "print('Best scores:')\n",
    "[(t.name, t.best_score) for t in trainers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bcede5-4332-41a9-a28a-9a39e2715891",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = ProgressPlot(\n",
    "    plot_names=[\"epsilon\", \"score\", \"moving average\", \"best score\",\"mean difficulty\", \"mean ticks\"],\n",
    "    line_names=[t.name for t in trainers],\n",
    "    width=2000,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "plot_log = []\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    eps = [t.epsilon() for t in trainers]\n",
    "    \n",
    "    step_info = [t.step() for t in trainers]\n",
    "    \n",
    "    score = [s[0] for s in step_info]\n",
    "    moving_avg = [s[1] for s in step_info]\n",
    "    \n",
    "    best = [t.best_score for t in trainers]\n",
    "    \n",
    "    stats = [t.agent.memory_stats() for t in trainers]\n",
    "    diff = [s['mean_difficulty'] for s in stats]\n",
    "    ticks = [s['mean_ticks'] for s in stats]\n",
    "    \n",
    "    plot_log.append(list(np.dstack((eps, score, moving_avg, best, diff, ticks))))\n",
    "    \n",
    "    pp.update([eps, score, moving_avg, best, diff, ticks])\n",
    "\n",
    "pp.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df709c5f-d8fd-492c-9fa5-62d118647a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    done = False\n",
    "    state = game.reset()\n",
    "    while not done:\n",
    "        action = agent.act(state, False)\n",
    "        state, _, done = game.step(action)\n",
    "        game.draw()รง\n",
    "        time.sleep(0.05)\n",
    "    print(game.score())\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02b882-3872-4505-b0d3-4cc64e8d57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to onnx\n",
    "\n",
    "t0 = Trainer(\n",
    "        Game(9, 9),\n",
    "        hidden_layer_size=512,\n",
    "        hidden_layers=3,\n",
    "        mem_batch=4096,\n",
    "        load=True,\n",
    "        epsilon=0.1\n",
    ")\n",
    "\n",
    "agent.model.eval()\n",
    "torch.onnx.export(\n",
    "    t0.agent.model, \n",
    "    torch.randn(1, 81), \n",
    "    \"snake.onnx\",\n",
    "    export_params=True,        # store the trained parameter weights inside the model file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb70fb6-3654-41c9-83ca-2f12adb68e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.state().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4b6a3-f712-47ee-b2cb-feecb7644ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
